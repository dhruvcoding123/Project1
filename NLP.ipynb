{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271b16f0-43f5-477f-8740-80d7c0206fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DHRUV\n"
     ]
    }
   ],
   "source": [
    "#NLP INTRO\n",
    "a = \"DHRUV\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed07723-92de-4507-bdad-bf19eb7337ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R\n"
     ]
    }
   ],
   "source": [
    "a = \"DATAFRAME\"\n",
    "print(a[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d65aa4a-0335-49e9-93d4-a1f4d800d0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta F\n",
      "--------------------\n",
      "Fra\n",
      "--------------------------------------------------\n",
      "aaF\n"
     ]
    }
   ],
   "source": [
    "a = \"Data FraME\"\n",
    "print(a[2:6])\n",
    "print(\"-\" * 20)\n",
    "print(a[-5:-2])\n",
    "print(\"-\" * 50)\n",
    "print(a[1:6:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce9fdce-4324-4465-8fb2-0f45f5999d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'FraME']\n"
     ]
    }
   ],
   "source": [
    "print(a.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5f81c6-6fd4-4209-871b-faf0e4868ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame\n",
      "DATA FRAME\n",
      "low FraME\n"
     ]
    }
   ],
   "source": [
    "print(a.lower())\n",
    "print(a.upper())\n",
    "print(a.replace(\"Data\" , \"low\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22bceb1d-c39f-4bf5-8df7-df7153bae2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame train\n"
     ]
    }
   ],
   "source": [
    "a = \"data frame\"\n",
    "b = \"train\"\n",
    "print(a + \" \" + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35628380-da76-47e9-9d10-7d21ae2b736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('dataset1.txt' , \"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed00cb01-e7ea-47d8-99d9-0341fd63c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('dataset2.txt' , \"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f28d9f67-a08d-4f94-a329-4b4954c8d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "  file1.write(\"Line number is %d\\r\\n\" % (i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71c510ba-2de4-43ac-96cc-70faa78af9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b33aa2c-2323-422a-ac70-e4510e4fbe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('dataset2.txt' , \"a+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62dd2b19-4ef4-496a-ae42-aa020ef0778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f45c6a05-0bc3-4576-8ebd-14dd8d989578",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('dataset2.txt' , \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bd10d37-b213-4f72-9c53-d40ceca9abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line number is 1\n",
      "\n",
      "Line number is 2\n",
      "\n",
      "Line number is 3\n",
      "\n",
      "Line number is 4\n",
      "\n",
      "Line number is 5\n",
      "\n",
      "Line number is 6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if file1.mode == 'r':\n",
    "  contents = file1.read()\n",
    "  print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2c997a6-5b70-4859-a0a7-109ce64e93a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "     \n",
    "\n",
    "# Text lowercase\n",
    "def lowercase_text(text):\n",
    "  return text.lower()\n",
    "\n",
    "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
    "lowercase_text(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e2e2a1d-5ae8-4bf0-b809-65e9a7168a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are  balls in this bag, and  in the other one.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "  result = re.sub(r'\\d+' , '' , text)\n",
    "  return result\n",
    "\n",
    "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
    "remove_numbers(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f58a32f-f847-4e83-900e-a4a83edb383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflect\n",
      "  Downloading inflect-7.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from inflect) (10.1.0)\n",
      "Collecting typeguard>=4.0.1 (from inflect)\n",
      "  Downloading typeguard-4.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from typeguard>=4.0.1->inflect)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading inflect-7.4.0-py3-none-any.whl (34 kB)\n",
      "Downloading typeguard-4.3.0-py3-none-any.whl (35 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, typeguard, inflect\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "Successfully installed inflect-7.4.0 typeguard-4.3.0 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca5cf4c4-0891-4f19-8679-8b4c1b578553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are twenty-one bikes in this parking'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inflect\n",
    "q = inflect.engine()\n",
    "\n",
    "# convert number into text\n",
    "def convert_number(text):\n",
    "  temp_string = text.split()\n",
    "  new_str = []\n",
    "\n",
    "  for word in temp_string:\n",
    "    if word.isdigit():\n",
    "      temp = q.number_to_words(word)\n",
    "      new_str.append(temp)\n",
    "\n",
    "    else:\n",
    "      new_str.append(word)\n",
    "\n",
    "  temp_str = ' '.join(new_str)\n",
    "  return temp_str\n",
    "\n",
    "input_str = \"There are 21 bikes in this parking\"\n",
    "convert_number(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfc8533b-4015-4700-a2bc-e6339a44f122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Are you excited '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "  translator = str.maketrans('' , '' , string.punctuation)\n",
    "  return text.translate(translator)\n",
    "\n",
    "input_str = \"Hey, Are you excited? :)\"\n",
    "remove_punctuation(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68659943-a572-4d00-9022-bd13a9d9f89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9d96264-f40e-44d0-8dd2-441279005edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Remove default stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abdbb12a-c63a-41c0-9dea-75f6befca21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "365f8298-74b0-4645-b9d9-520409f60daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords function\n",
    "def remove_stopwords(text):\n",
    "  stop_words = set(stopwords.words(\"english\"))\n",
    "  word_tokens = word_tokenize(text)\n",
    "  filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "  return filtered_text\n",
    "\n",
    "ex_text = \"This is an example showing off stop word filtration.\"\n",
    "remove_stopwords(ex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c59e940-3a31-4116-b41f-257e0bea73b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# stem words in the list of tokenized words\n",
    "def stem_words(text):\n",
    "  word_tokens = word_tokenize(text)\n",
    "  stems = [stemmer.stem(word) for word in word_tokens]\n",
    "  return stems\n",
    "\n",
    "text = 'data science uses scientific methods algorithms and many types of processes'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d74d121-1c3c-41d0-b082-1d1277050f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fd679cc-dd14-4d69-8f28-de9f04fe6e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'science', 'be', 'a', 'special', 'branch', 'of', 'AI', 'AND', 'ML']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemma = wordnet.WordNetLemmatizer()\n",
    "\n",
    "# lemmatize string\n",
    "def lemmatize_word(text):\n",
    "  word_tokens = word_tokenize(text)\n",
    "  lemmas = [lemma.lemmatize(word , pos = 'v') for word in word_tokens]\n",
    "  return lemmas\n",
    "\n",
    "text = 'data science is a special branch of AI AND ML'\n",
    "lemmatize_word(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e9bd250-3ae5-4e9e-a12c-c42d78495661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('working', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('11', 'CD'),\n",
       " ('hours', 'NNS')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tagging(text):\n",
    "  word_tokens = word_tokenize(text)\n",
    "  return pos_tag(word_tokens)\n",
    "\n",
    "pos_tagging('I am working for 11 hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2dc8579-3386-471c-8d35-0cb55a4c76d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('PRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84eec77e-3aa5-430a-9baf-1d7437e1c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16f0d574-58f2-473c-a611-cf17a79bc880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S green/JJ lizard/NN is/VBZ noy/RB a/DT chameleon/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def ner(text):\n",
    "  word_tokens = word_tokenize(text)\n",
    "  word_pos = pos_tag(word_tokens)\n",
    "  print(ne_chunk(word_pos))\n",
    "\n",
    "text = 'green lizard is noy a chameleon'\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc4e2632-3b77-4cc6-ac01-afae8cccf533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sent = \"dataset, Data is the new fire\"\n",
    "r2 = re.findall(r\"^\\w+\", sent)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6827213-a57c-40f4-91c7-612e77fe9379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data is,  the new fire']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.split(r\"^\\s\", \"Data is,  the new fire\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07ab7cc9-3f29-4a86-aafa-a88ab74e609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n",
      "life\n",
      "is\n",
      "long\n"
     ]
    }
   ],
   "source": [
    "# Using re.match()\n",
    "lists =  ['my', 'life', 'is', 'long']\n",
    "\n",
    "for i in lists:\n",
    "  q = re.match(r\"^\\w+\", i)\n",
    "  if q:\n",
    "    print(q.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f49442d-1ad0-4280-b8e4-2c375d6cf254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'see', 'how', 'you', 'are', 'working']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text = \"Let's see how you are working.\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b20aa0b-270f-49f6-8781-8d9a4f1eed55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'see', 'how', 'you', 'are', 'working']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bef9f237-269d-4026-bef4-9be838c6ff1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70ffc192-3966-422c-8e73-92588ac28c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you', 'how', 'see', 'working', 's', 'are', 'Let'}\n"
     ]
    }
   ],
   "source": [
    "my_vocab = set(tokens)\n",
    "print(my_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25e74d1a-ee5a-4c78-a3db-ff69b7132c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ther',\n",
       " 'is',\n",
       " 'no',\n",
       " 'need',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'alone',\n",
       " ',',\n",
       " 'we',\n",
       " 'are',\n",
       " 'united',\n",
       " 'together']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_st = \"Ther is no need to feel alone, we are united together\"\n",
    "\n",
    "     \n",
    "\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "     \n",
    "\n",
    "m_t = WordPunctTokenizer().tokenize(my_st)\n",
    "m_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d9af037-84fe-448e-8b2c-09ddf82a4410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends, hope upi is there for you all']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Hello friends, hope upi is there for you all\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11c2dda9-597a-449f-b7b3-8e866e2d9c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "<FreqDist with 11 samples and 12 outcomes>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"\\n\\n\\n\")\n",
    "text1 = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article\"\n",
    "fd = nltk.FreqDist(word_tokenize(text1))\n",
    "print(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54f36303-a024-46c2-9e4d-ff7b66ec89a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'dict_keys'>\n"
     ]
    }
   ],
   "source": [
    "print(fd['everyone'])\n",
    "words = fd.keys()\n",
    "print(type(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c32ca-c26d-493e-8b41-21b8cac72d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
